\setcounter{chapter}{2}
\chapter{METHODOLOGY}

\section{Introduction}
This research implements the Cross-Industry Standard Process for Data Mining (CRISP-DM) framework to develop Eyeway 2.0, a vehicle-mounted pothole detection and quantification system. The methodology enables systematic development and validation of real-time road condition monitoring capabilities, transitioning from high-level business requirements to deployable AIoT solutions.

\section{Research Framework}
The CRISP-DM model structures the research into six iterative phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment. Figure \ref{fig:crisp-dm} illustrates the framework implementation adapted for the Eyeway 2.0 lifecycle.

\begin{figure}[h!]
  \centering
  \tikzset{
    % CRISP-DM diagram styles (harmonized color scheme)
    crispnode/.style={circle, draw=blue!60!black, very thick, fill=blue!8, minimum size=2.5cm, align=center, font=\small},
    centerdata/.style={circle, draw=teal!70!black, very thick, fill=teal!15, minimum size=1.8cm, align=center, font=\small},
    outerarrow/.style={-Stealth, very thick, blue!60!black},
    innerarrow/.style={-Stealth, thick, teal!60!black},
    backarrow/.style={-Stealth, thick, dashed, blue!40},
  }
  \begin{tikzpicture}[node distance=1.2cm]
    % Center Data
    \node[centerdata] (data) at (0,0) {\textbf{Data}};

    % Nodes arranged in a circle (outer ring)
    \node[crispnode] (bu)   at (90:3.5cm)  {Business\\Understanding};
    \node[crispnode] (du)   at (30:3.5cm)  {Data\\Understanding};
    \node[crispnode] (dp)   at (-30:3.5cm) {Data\\Preparation};
    \node[crispnode] (mod)  at (-90:3.5cm) {Modeling};
    \node[crispnode] (eval) at (-150:3.5cm){Evaluation};
    \node[crispnode] (dep)  at (150:3.5cm) {Deployment};

    % Arrows forming the outer cycle (solid)
    \draw[outerarrow] (bu)  to[bend left=20] (du);
    \draw[outerarrow] (du)  to[bend left=20] (dp);
    \draw[outerarrow] (dp)  to[bend left=20] (mod);
    \draw[outerarrow] (mod) to[bend left=20] (eval);
    \draw[outerarrow] (eval)to[bend left=20] (dep);
    \draw[outerarrow] (dep) to[bend left=20] (bu);

    % Backwards/iterative arrows (dashed)
    \draw[backarrow] (du)   to[bend left=20] (bu);
    \draw[backarrow] (mod)  to[bend left=20] (dp);
    \draw[backarrow] (eval) to[bend left=20] (bu);

    % Central interaction lines (bidirectional)
    \draw[innerarrow] (data) -- (mod);
    \draw[innerarrow] (mod)  -- (data);
    \draw[innerarrow] (data) -- (du);
    \draw[innerarrow] (du)   -- (data);
    \draw[innerarrow] (data) -- (dp);
    \draw[innerarrow] (dp)   -- (data);

  \end{tikzpicture}
  \caption{Research framework implementation based on CRISP-DM for the pothole detection system.}
  \label{fig:crisp-dm}
\end{figure}

\section{Business Understanding Phase}

\subsection{System Requirements}
\label{sec:system_requirements}

Based on standards from the Department of Public Works and Highways (DPWH), the core system requirements for the automated visual inspection system were established. The camera mounting parameters specify an optimal mounting height ($H$) ranging between 2.1 and 3.5 meters above the road surface to simulate the vantage point of trucks or utility vehicles. To maximize road surface coverage while minimizing horizon interference, the camera angle ($\theta$) is set to a 30$^\circ$--45$^\circ$ downward tilt from the horizontal axis. Furthermore, the specifications require corrosion-resistant aluminum alloy (Grade 6061-T6) brackets equipped with a 3-axis adjustable mounting plate to withstand vibrational fatigue during operation.

Regarding processing capabilities, the system mandates a minimum resolution of 12 MP per camera, which is downsampled during inference to capture fine texture details. Real-time image processing is required to maintain a total system latency of less than 500 ms, thereby supporting vehicle speeds up to 60 km/h. To handle the computational load of transformer-based monocular depth inference, the architecture necessitates NVIDIA Jetson-class GPU support. Table \ref{tab:hardware_specs} summarizes the hardware components of the Eyeway 2.0 system.

\begin{table}[htbp]
\caption{Hardware Components Specification}
\label{tab:hardware_specs}
\begin{tabularx}{\textwidth}{X l l}
\toprule
Component & Specification & Purpose \\ 
\midrule
Edge Computer & NVIDIA Jetson Orin Nano (8GB) & Real-time AI inference \\
Camera & 12 MP RGB (IMX477 sensor) & Road surface capture \\
GPS Module & u-blox NEO-M8N (NMEA 0183) & Geospatial tagging \\
Storage & 256GB NVMe SSD & Data logging \\
Power & 12V DC (vehicle adapter) & Continuous operation \\
Mount & Aluminum suction bracket & Windshield attachment \\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure}[h!]
  \centering
\begin{tikzpicture}[scale=1.1, >=Latex]

  % ============================================================
  % CONFIGURATION VARIABLES - Adjust these to modify the diagram
  % ============================================================
  
  % --- Pothole Position ---
  \def\potholeX{8}        % X position of pothole center
  \def\potholeY{-0.35}    % Y position of pothole
  
  % --- Dashcam Position ---
  \def\camX{3.05}         % X position of dashcam
  \def\camY{1.27}         % Y position of dashcam (camera height from road)
  
  % --- FOV Endpoints ---
  \def\fovUpperX{9.5}     % X position of upper FOV ray endpoint
  \def\fovUpperY{0.6}     % Y position of upper FOV ray endpoint
  \def\fovLowerX{7.5}       % X position of lower FOV ray endpoint
  \def\fovLowerY{-0.5}    % Y position of lower FOV ray endpoint
  
  % --- Detection Distance ---
  \def\distLineY{-1.15}   % Y position of detection distance arrow
  
  % --- Road Boundaries ---
  \def\roadLeft{-1.5}
  \def\roadRight{10}

  % ============================================================
  % ROAD SURFACE
  % ============================================================
  \fill[gray!20] (\roadLeft,-0.5) rectangle (\roadRight,0);
  \draw[thick, gray!70] (\roadLeft,0) -- (\roadRight,0);
  \node[anchor=north west, font=\small, gray!80] at (8.5,-0.1) {Road Surface};

  % ============================================================
  % POTHOLE
  % ============================================================
  \fill[gray!50] (\potholeX,\potholeY) ellipse (0.7 and 0.2);
  \draw[thick, gray!70] (\potholeX-0.7,-0.15) arc (180:360:0.7 and 0.32);
  \node[anchor=north, font=\small] at (\potholeX,-0.5) {Pothole};

  % ============================================================
  % SEDAN
  % ============================================================
  \begin{scope}[shift={(0,0)}]
    \fill[gray!15] (0,0.25) rectangle (4.5,0.75);
    \draw[gray!60, thick] (0,0.25) -- (0,0.75) -- (4.5,0.75) -- (4.5,0.25);
    \fill[gray!15] (0.7,0.75) -- (1.1,1.4) -- (3.4,1.4) -- (3.8,0.75) -- cycle;
    \draw[gray!60, thick] (0.7,0.75) -- (1.1,1.4) -- (3.4,1.4) -- (3.8,0.75);
    \fill[teal!20] (1.15,0.82) -- (1.45,1.32) -- (2.15,1.32) -- (2.15,0.82) -- cycle;
    \draw[teal!50, thick] (1.15,0.82) -- (1.45,1.32) -- (2.15,1.32) -- (2.15,0.82) -- cycle;
    \fill[teal!20] (2.25,0.82) -- (2.25,1.32) -- (3.0,1.32) -- (3.35,0.82) -- cycle;
    \draw[teal!50, thick] (2.25,0.82) -- (2.25,1.32) -- (3.0,1.32) -- (3.35,0.82) -- cycle;
    \draw[gray!60, thick] (0,0.25) -- (0.5,0.25);
    \draw[gray!60, thick] (1.5,0.25) -- (3.0,0.25);
    \draw[gray!60, thick] (4.0,0.25) -- (4.5,0.25);
    \fill[gray!70] (1.0,0) circle (0.35);
    \fill[gray!70] (3.5,0) circle (0.35);
    \fill[gray!40] (1.0,0) circle (0.2);
    \fill[gray!40] (3.5,0) circle (0.2);
  \end{scope}

  % ============================================================
  % DASHCAM
  % ============================================================
  \fill[red!70] (2.8, 1.18) rectangle (\camX, 1.35);
  \draw[red!90, thick] (2.8, 1.18) rectangle (\camX, 1.35);
  \node[anchor=south, font=\scriptsize\bfseries, text=red!70!black] at (2.925, 1.38) {Dashcam};

  % ============================================================
  % CAMERA HEIGHT (H)
  % ============================================================
  \draw[<->, thick] (-1.0,0) -- (-1.0,\camY);
  \node[anchor=east, font=\small] at (-1.05,0.63) {$H$};
  \draw[dashed, gray] (-1.0,\camY) -- (2.8,\camY);

  % ============================================================
  % HORIZONTAL REFERENCE
  % ============================================================
  \draw[dashed, gray!60] (\camX,\camY) -- (5.5,\camY);
  \node[anchor=south west, font=\scriptsize, gray!60] at (4.5,\camY) {Horizontal};

  % ============================================================
  % OPTICAL AXIS (RED ARROW)
  % ============================================================
  \draw[red!70, line width=1.5pt, ->] (\camX,\camY) -- (\potholeX,0);

  % ============================================================
  % FIELD OF VIEW (FOV) - Orange rays
  % ============================================================
  \fill[orange!10, opacity=0.5] (\camX,\camY) -- (\fovUpperX,\fovUpperY) -- (\fovLowerX,\fovLowerY) -- cycle;
  \draw[orange!60, dashed, thick] (\camX,\camY) -- (\fovUpperX,\fovUpperY);
  \draw[orange!60, dashed, thick] (\camX,\camY) -- (\fovLowerX,\fovLowerY);
  \node[font=\small, orange!70!black] at (7.2,0.8) {FOV};

  % ============================================================
  % DEPRESSION ANGLE (θ)
  % ============================================================
  \draw[red!70, thick, ->] (5.0,\camY) arc (0:-15:1.95);
  \node[anchor=west, font=\normalsize, red!70!black] at (5.1,1.0) {$\theta$};

  % ============================================================
  % DETECTION DISTANCE (D)
  % ============================================================
  \draw[dashed, gray!50] (\camX,0) -- (\camX,\distLineY-0.15);
  \draw[dashed, gray!50] (\potholeX,0) -- (\potholeX,\distLineY-0.15);
  \draw[<->, thick] (\camX,\distLineY) -- (\potholeX,\distLineY);
  \node[fill=white, inner sep=2pt, font=\small] at ({(\camX+\potholeX)/2},\distLineY) {Detection Distance $D$};

  \end{tikzpicture}
  \caption{Dashcam geometry in a compact sedan showing depression angle ($\theta$), camera height ($H$), and detection distance ($D$) to a pothole.}
  \label{fig:mounting_schematics}
\end{figure}

The specified 30$^\circ$--45$^\circ$ camera angle ensures optimal field coverage while minimizing perspective distortion, which is critical for the accuracy of monocular depth estimation. The heavy-duty suction mount provides flexible deployment capabilities while maintaining structural stability through vacuum adhesion, keeping maximum vibration deflection within acceptable limits for rolling shutter sensors.

\section{System Architecture}
The Eyeway 2.0 system implements a three-layer AIoT architecture comprising the Physical \& Edge Layer, Network Layer, and Application Layer. This layered design enables real-time on-vehicle processing with cloud-based visualization for end users. Figure \ref{fig:system_architecture} illustrates the complete system architecture.

\begin{figure}[ht!]
  \centering
  \resizebox{1\textwidth}{!}{%
  \begin{tikzpicture}[>=Latex,
    % === BOX SIZING (adjust these to change all boxes) ===
    box/.style={draw, rectangle, thick, align=center, font=\footnotesize,
      minimum width=\boxW, minimum height=\boxH, inner xsep=5pt, inner ysep=4pt},
    hw/.style={box, draw=gray!60, fill=gray!10},
    proc/.style={box, draw=blue!60!black, fill=blue!10},
    net/.style={box, draw=teal!70!black, fill=teal!15, rounded corners=3pt},
    app/.style={box, draw=orange!70!black, fill=orange!12, rounded corners=3pt},
    arr/.style={-Stealth, thick, shorten <=2pt, shorten >=2pt}
  ]

  % ============================================================
  % CONFIGURATION VARIABLES - Adjust these to modify the diagram
  % ============================================================
  
  % --- Box Dimensions ---
  \def\boxW{2.2cm}    % Box width
  \def\boxH{0.85cm}   % Box height
  
  % --- Column Positions (X) ---
  \def\colA{0}        % Input column
  \def\colB{4.9}      % Processing column
  \def\colC{9}        % Output column
  
  % --- Row Positions (Y) ---
  \def\rowCam{-0.5}   % Camera row
  \def\rowYolo{0.3}   % YOLO row
  \def\rowDA{-1.3}    % Depth Anything row
  \def\rowBot{-2.3}   % GPS/Logger row
  \def\rowLower{-5.2} % Bottom layer row
  
  % --- Arrow Junction Points ---
  \def\juncA{2}       % Camera split junction X
  \def\juncB{7}       % Fusion merge junction X
  
  % --- Edge Layer Boundaries ---
  \def\edgeL{-1.8}    % Left
  \def\edgeR{10.8}    % Right
  \def\edgeT{2.2}     % Top
  \def\edgeB{-3.5}    % Bottom
  
  % --- Jetson Box Boundaries ---
  \def\jetL{2.5}      % Left
  \def\jetR{10.5}     % Right
  \def\jetT{1.5}      % Top
  \def\jetB{-3}     % Bottom
  
  % --- Bottom Layers Boundaries ---
  \def\botT{-4}       % Top of bottom layers
  \def\botB{-6.5}     % Bottom of bottom layers
  \def\appR{4.0}      % Application layer right edge
  \def\netL{4.3}      % Network layer left edge
  
  % --- Label Positions ---
  \def\lblEdgeX{0.4}    \def\lblEdgeY{1.75}
  \def\lblJetX{6.7}   \def\lblJetY{1.15}
  \def\lblAppX{0}   \def\lblAppY{-6.25}
  \def\lblNetX{5.85}   \def\lblNetY{-6.25}

  % ============================================================
  % NODES
  % ============================================================
  \node[hw] (cam) at (\colA,\rowCam) {RGB Camera\\(12 MP)};
  \node[hw] (gps) at (\colA,\rowBot) {GPS Module\\(NMEA 0183)};
  \node[proc] (yolo) at (\colB,\rowYolo) {YOLOv9-tiny\\Detection};
  \node[proc] (da3) at (\colB,\rowDA) {Depth Anything V3\\Metric Depth};
  \node[proc] (fusion) at (\colC,\rowCam) {Data Fusion\\Area Calc};
  \node[proc] (logger) at (\colC,\rowBot) {Data Logger\\(JSON)};
  \node[app] (dash) at (1.2,\rowLower) {Web Dashboard\\Geospatial Map};
  \node[net] (supa) at (9,\rowLower) {Supabase\\Cloud Database};

  % ============================================================
  % LAYER BACKGROUNDS
  % ============================================================
  \begin{scope}[on background layer]
    \fill[gray!8, rounded corners=6pt] (\edgeL,\edgeT) rectangle (\edgeR,\edgeB);
    \draw[gray!50, line width=1.5pt, rounded corners=6pt] (\edgeL,\edgeT) rectangle (\edgeR,\edgeB);
    \fill[blue!6, rounded corners=5pt] (\jetL,\jetT) rectangle (\jetR,\jetB);
    \draw[blue!60, dashed, line width=1.5pt, rounded corners=5pt] (\jetL,\jetT) rectangle (\jetR,\jetB);
    \fill[orange!8, rounded corners=6pt] (\edgeL,\botT) rectangle (\appR,\botB);
    \draw[orange!50, line width=1.5pt, rounded corners=6pt] (\edgeL,\botT) rectangle (\appR,\botB);
    \fill[teal!8, rounded corners=6pt] (\netL,\botT) rectangle (\edgeR,\botB);
    \draw[teal!50, line width=1.5pt, rounded corners=6pt] (\netL,\botT) rectangle (\edgeR,\botB);
  \end{scope}

  % ============================================================
  % LAYER LABELS
  % ============================================================
  \node[font=\bfseries\small, gray!60!black] at (\lblEdgeX,\lblEdgeY) {Physical \& Edge Layer};
  \node[font=\bfseries\small, blue!60!black] at (\lblJetX,\lblJetY) {Jetson Orin Nano};
  \node[font=\bfseries\small, orange!60!black] at (\lblAppX,\lblAppY) {Application Layer};
  \node[font=\bfseries\small, teal!60!black] at (\lblNetX,\lblNetY) {Network Layer};

  % ============================================================
  % ARROWS (using junction variables)
  % ============================================================
  \draw[arr, blue!60] (cam.east) -- (\juncA,\rowCam) |- (yolo.west);
  \draw[arr, blue!60] (cam.east) -- (\juncA,\rowCam) |- (da3.west);
  \draw[arr, blue!60] (yolo.east) -- (\juncB,\rowYolo) -- (\juncB,-0.35) -- ([yshift=4pt]fusion.west);
  \draw[arr, blue!60] (da3.east) -- (\juncB,\rowDA) -- (\juncB,-0.62) -- ([yshift=-4pt]fusion.west);
  \draw[arr, blue!60] (fusion.south) -- (logger.north);
  \draw[arr, blue!60] (gps.east) -- (logger.west);
  \draw[arr, teal!70] (logger.south) -- (supa.north) node[right, pos=0.5, font=\footnotesize\bfseries, teal!70!black]{WiFi};
  \draw[arr, orange!70] (supa.west) -- (dash.east) node[above, pos=0.4, font=\footnotesize\bfseries, orange!70!black]{Real-time Sync};

  \end{tikzpicture}%
  }
  \caption{Three-layer AIoT system architecture for Eyeway 2.0.}
  \label{fig:system_architecture}
\end{figure}

\section{Data Understanding Phase}

\subsection{Dataset Development}
To ensure the model generalizes to tropical road conditions, training protocols established specific image quality requirements. The dataset is composed of primary data, consisting of 2,000 images collected locally under varying lighting conditions such as overcast and direct sunlight, covering both asphalt and concrete pavement types. Additionally, secondary data comprising 2,000 augmented images from the Road Damage Dataset (RDD2022) is included to introduce variability.

\subsection{Operating Parameters}
Empirical testing quantified speed-performance relationships. The system is calibrated for optimal detection accuracy at vehicle speeds between 20 km/h and 60 km/h. Testing validated that motion blur at these speeds is negligible for the YOLOv9-tiny model.

\section{Data Preparation Phase}

\subsection{Image Processing Protocol}
Standardization procedures maintain measurement accuracy through calibrated processing. The intrinsic camera matrix ($K$) is extracted using a standard checkerboard pattern to correct radial distortion ($k_1, k_2, k_3$) and tangential distortion ($p_1, p_2$). Subsequently, input frames are resized to $640 \times 640$ for YOLOv9-tiny inference and $518 \times 518$ for Depth Anything V3 to ensure aspect ratio preservation.

\subsection{Spatial Reference System}
GPS integration protocols synchronize detection frames with NMEA 0183 location data. This enables precise pothole mapping and spatial validation against existing road network maps.

\section{Modeling Phase}

\subsection{Model Selection via Explainability Analysis}
\label{sec:model_selection_xai}

The selection of an appropriate detection architecture extends beyond traditional performance metrics. For systems that quantify pothole surface area, the model must accurately identify the complete spatial extent of each defect. This requirement motivated an interpretability-driven selection process using LayerCAM visualization \parencite{Jiang2021LayerCAM}.

LayerCAM was adapted specifically for YOLO architectures by generating class activation maps from the final three convolutional layers of each model's backbone. For class-agnostic gradient computation, a pseudo-score was calculated as the sum of all activations in the output feature maps. The resulting CAMs were averaged to form stable attention representations.

Three lightweight models were evaluated: YOLOv9-tiny, YOLOv10-nano, and YOLOv11-nano. The analysis employed three complementary metrics. CAM IoU measures spatial alignment between the model's attention heatmap and ground-truth pothole boundaries, where a higher score indicates the model ``sees'' the complete object shape. Pointing Accuracy assesses whether the maximum activation point falls within the ground-truth region, reflecting localization precision. Energy Ratio calculates the proportion of attention energy concentrated within the object region versus background.

The interpretability analysis revealed that YOLOv9-tiny achieves a mean CAM IoU of 0.674, over four times higher than YOLOv10-nano (0.160) and eight times higher than YOLOv11-nano (0.082) \parencite{Fortin2025XAI}. This holistic attention pattern ensures complete coverage of pothole boundaries, which is essential for the surface area calculation in Equation \ref{eq:surface_area}. While YOLOv10-nano and YOLOv11-nano demonstrate superior pointing accuracy (0.814 and 0.787 respectively), their concentrated ``pointer-like'' attention focuses on discriminative subregions rather than the full defect extent, potentially leading to underestimation of surface area.

Based on this analysis, YOLOv9-tiny was selected as the optimal architecture for Eyeway 2.0, balancing detection accuracy with the interpretability requirements of quantitative damage assessment.

\subsection{Detection System Development (YOLOv9-tiny)}
The system implements the YOLOv9-tiny architecture for real-time detection. YOLOv9-tiny introduces Programmable Gradient Information (PGI), which manages information flow through the network and mitigates information loss in deep feed-forward architectures \parencite{Wang2024a}. This enables learning of discriminative features with fewer parameters while maintaining the holistic attention pattern critical for surface area estimation.

\subsection{Monocular Metric Depth \& Surface Area}
The core innovation of Eyeway 2.0 is the quantification of damage. Depth Anything V3 (DA3) is used to predict the per-pixel metric distance $Z$. The real-world surface area ($SA$) is calculated via geometric back-projection:

\begin{equation}
\label{eq:surface_area}
  SA = \sum_{i \in \text{ROI}} \left( \frac{Z_i^2}{f_x \cdot f_y} \right)
\end{equation}

Where $Z_i$ is the depth of the $i$-th pixel within the pothole mask, and $f_x, f_y$ are the camera focal lengths. This formula converts the 2D area into physical units ($cm^2$), addressing the limitations of purely bounding-box based approaches. The accuracy of this calculation depends critically on the detection model's ability to correctly identify all pixels within the pothole region, which is ensured by YOLOv9-tiny's holistic attention mechanism. Figure \ref{fig:surface_area_calc} illustrates the geometric back-projection concept.

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}[>=Latex, scale=0.85]
    % Image plane
    \draw[thick, fill=blue!5] (0,0) rectangle (4,3.5);
    \node[font=\small\bfseries, blue!60!black] at (2,3.8) {Image Plane};
    
    % Detected pothole region (ROI) - more realistic irregular circular pothole shape
    \draw[thick, fill=orange!30, draw=orange!70] 
      (1.2,1.2) .. controls (1.0,1.6) and (1.3,2.0) .. (1.8,2.1)
      .. controls (2.3,2.2) and (2.8,2.0) .. (3.0,1.6)
      .. controls (3.1,1.2) and (2.9,0.8) .. (2.4,0.7)
      .. controls (1.9,0.6) and (1.4,0.8) .. (1.2,1.2);
    \node[font=\scriptsize, orange!70!black] at (2.1,1.4) {ROI};
    
    % Pixel grid indication
    \draw[gray!40, very thin, step=0.3] (1.0,0.6) grid (3.1,2.2);
    
    % Arrow to real world
    \draw[-Stealth, very thick, teal!70] (4.8,1.75) -- (6.5,1.75);
    \node[font=\scriptsize, teal!70!black, align=center] at (5.65,2.5) {Geometric\\Back-projection};
    
    % Real world representation
    \draw[thick, fill=gray!20] (7,0) rectangle (12,3.5);
    \node[font=\small\bfseries, gray!60!black] at (9.5,3.8) {Real World};
    
    % Real pothole (scaled larger) - more realistic irregular circular shape
    \draw[thick, fill=orange!30, draw=orange!70] 
      (7.8,1.2) .. controls (7.5,1.8) and (7.9,2.4) .. (8.8,2.6)
      .. controls (9.7,2.8) and (10.6,2.4) .. (10.9,1.7)
      .. controls (11.1,1.0) and (10.7,0.5) .. (9.8,0.4)
      .. controls (8.9,0.3) and (8.0,0.6) .. (7.8,1.2);
    
    % Dimension annotations - moved inside
    \draw[<->, thick] (7.8,2.9) -- (10.9,2.9);
    \node[font=\scriptsize, fill=white, inner sep=2pt] at (9.35,2.9) {Width (cm)};
    
    % Formula reference - moved down
    \node[font=\scriptsize, align=center, fill=white, inner sep=2pt] at (10.8,-0.5) {$SA = \sum \frac{Z_i^2}{f_x \cdot f_y}$};
    
    % Depth annotation - single direction arrow showing distance
    \draw[dashed, gray!60] (2,-0.3) -- (2,-1.2);
    \draw[dashed, gray!60] (9.5,-0.3) -- (9.5,-1.2);
    \draw[-Stealth, thick, gray!60] (2,-0.9) -- (9.5,-0.9);
    \node[font=\scriptsize, gray!60, fill=white, inner sep=2pt] at (5.75,-0.9) {Depth $Z$ (from Depth Anything V3)};
    
  \end{tikzpicture}
  \caption{Surface area calculation concept: detected pothole pixels in image plane are back-projected to real-world dimensions using depth information and camera intrinsics.}
  \label{fig:surface_area_calc}
\end{figure}

Figure \ref{fig:processing_pipeline} illustrates the complete data processing pipeline from image capture to quantified output.

\begin{figure}[ht!]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tikzpicture}[>=Latex,
    box/.style={draw, rectangle, thick, align=center, font=\footnotesize,
      minimum width=2cm, minimum height=1cm, inner xsep=8pt, inner ysep=6pt},
    input/.style={box, draw=gray!60, fill=gray!10, rounded corners=3pt},
    process/.style={box, draw=blue!60!black, fill=blue!10},
    output/.style={box, draw=teal!70!black, fill=teal!15, rounded corners=3pt},
    arr/.style={-Stealth, thick, shorten <=2pt, shorten >=2pt}
  ]
  
  % Input
  \node[input] (frame) at (0,0) {RGB Frame\\(640×640)};
  
  % Parallel processing
  \node[process] (yolo) at (3.5,0.8) {YOLOv9-tiny\\Detection};
  \node[process] (depth) at (3.9,-0.8) {Depth Anything V3\\Metric Depth};
  
  % Fusion
  \node[process] (fusion) at (8,0) {Data Fusion\\ROI Extraction};
  
  % Calculation
  \node[process] (calc) at (11.5,0) {Surface Area\\Calculation};
  
  % Output
  \node[output] (output) at (15,0) {Geotagged\\Detection\\+ Area ($cm^2$)};
  
  % GPS input
  \node[input] (gps) at (11.5,-1.8) {GPS\\Coordinates};
  
  % Arrows
  \draw[arr, blue!60] (frame.east) -- ++(0.5,0) |- (yolo.west);
  \draw[arr, blue!60] (frame.east) -- ++(0.5,0) |- (depth.west);
  \draw[arr, blue!60] (yolo.east) -- ++(0.5,0) |- ([yshift=3pt]fusion.west);
  \draw[arr, blue!60] (depth.east) -- ++(0.5,0) |- ([yshift=-3pt]fusion.west);
  \draw[arr, blue!60] (fusion.east) -- (calc.west);
  \draw[arr, blue!60] (calc.east) -- (output.west);
  \draw[arr, gray!60] (gps.north) -- (calc.south);
  
  % Labels
  \node[font=\scriptsize, blue!60!black] at (5.9,1.2) {Bounding Boxes};
  \node[font=\scriptsize, blue!60!black] at (6.4,-1.2) {Depth Map};
  \node[font=\scriptsize, gray!60] at (10.8,-1) {Location};
  
  \end{tikzpicture}%
  }
  \caption{Data processing pipeline showing parallel detection and depth estimation with fusion for surface area quantification.}
  \label{fig:processing_pipeline}
\end{figure}

\section{Evaluation Phase}

\subsection{Performance Assessment}
System evaluation quantifies detection accuracy and processing efficiency using specific metrics. Detection performance is measured using Mean Average Precision (mAP@50). Quantification accuracy is assessed via Mean Absolute Error (MAE), comparing the predicted surface area against ground-truth measurements taken manually with a tape measure. Finally, system speed is evaluated by recording the average Frames Per Second (FPS) on the Jetson edge device.

\subsection{Integration Testing}
Field validation confirmed the reliable integration of detection, depth estimation, and GPS tagging. Testing verified system stability under continuous operation for 2-hour inspection loops.

\section{Deployment Phase}

\subsection{Installation Requirements}
Implementation procedures specify installation requirements, field testing protocols, and operational guidelines. The methodology addresses system maintenance, such as cleaning the lens and checking bracket tightness, alongside operator training requirements to ensure consistent data collection.

\subsection{Cloud Infrastructure Development}
Cloud-based systems enable real-time mapping and data management. The Eyeway 2.0 system utilizes Supabase as a serverless Backend-as-a-Service (BaaS) platform, eliminating the need for dedicated server infrastructure while providing scalable database, real-time synchronization, and API capabilities.

The cloud architecture leverages four key Supabase components. The PostgreSQL Database stores geotagged detection records including coordinates, timestamps, surface area measurements, and severity classifications. Realtime Subscriptions enable live updates to connected dashboards via WebSocket connections, allowing infrastructure managers to monitor new detections as they occur. The REST API provides secure endpoints for data ingestion from edge devices and data retrieval for visualization. Row Level Security (RLS) ensures data isolation between different LGU jurisdictions while maintaining a shared infrastructure.

Figure \ref{fig:cloud_architecture} illustrates the cloud infrastructure architecture and data flow between system components.

\begin{figure}[ht!]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tikzpicture}[>=Latex,
    box/.style={draw, rectangle, thick, align=center, font=\footnotesize,
      minimum width=2.5cm, minimum height=1.2cm, inner xsep=8pt, inner ysep=6pt},
    edge/.style={box, draw=gray!60, fill=gray!10, rounded corners=3pt},
    cloud/.style={box, draw=teal!70!black, fill=teal!15, rounded corners=5pt},
    supa/.style={box, draw=green!60!black, fill=green!10, rounded corners=3pt},
    app/.style={box, draw=orange!70!black, fill=orange!12, rounded corners=3pt},
    arr/.style={-Stealth, thick, shorten <=3pt, shorten >=3pt}
  ]
  
  % Edge Device Section
  \node[edge] (jetson) at (0,0) {Jetson Orin Nano\\(Edge Processing)};
  
  % Network
  \node[cloud, minimum width=2cm] (wifi) at (4,0) {WiFi\\Network};
  
  % Supabase Cloud (large container - wider)
  \begin{scope}[on background layer]
    \fill[green!5, rounded corners=10pt] (6.5,-3) rectangle (17,3);
    \draw[green!50, line width=2pt, rounded corners=10pt] (6.5,-3) rectangle (17,3);
  \end{scope}
  \node[font=\bfseries\small, green!60!black] at (11.75,2.5) {Supabase Cloud (Serverless)};
  
  % Supabase Components (spread out more)
  \node[supa] (api) at (8.5,1) {REST API\\(PostgREST)};
  \node[supa] (db) at (12,0) {PostgreSQL\\Database};
  \node[supa] (realtime) at (8.5,-1) {Realtime\\(WebSocket)};
  \node[supa] (rls) at (15.5,0) {Row Level\\Security};
  
  % Application Layer
  \node[app] (dashboard) at (20.5,0) {Web Dashboard\\(Geospatial Map)};
  
  % Arrows - Edge to Cloud (both use L-shaped routing)
  \draw[arr, gray!70] (jetson.east) -- (wifi.west) node[midway, above, font=\scriptsize] {JSON};
  \draw[arr, teal!70] (wifi.east) -- ++(1,0) node[above, font=\scriptsize, fill=white, inner sep=1pt] {HTTPS} |- (api.west);
  \draw[arr, teal!70] (wifi.east) -- ++(1,0) |- (realtime.west);
  
  % Arrows - Inside Supabase (adjusted for new positions)
  \draw[arr, green!60] (api.east) -- (db.north west);
  \draw[arr, green!60] (realtime.east) -- (db.south west);
  \draw[arr, green!60] (db.east) -- (rls.west);
  
  % Arrows - Cloud to Apps
  \draw[arr, orange!70] (rls.east) -- (dashboard.west) node[midway, above, font=\scriptsize] {Live Updates};
  
  % Labels
  \node[font=\scriptsize, gray!60] at (0,-2) {Physical Layer};
  \node[font=\scriptsize, teal!60] at (4,-2) {Network Layer};
  \node[font=\scriptsize, orange!60] at (20.5,-1.5) {Application Layer};
  
  % Data flow annotations
  \node[font=\scriptsize, green!60!black, align=center] at (12,-2.3) {Pothole Records: lat, lng, area, severity};
  
  \end{tikzpicture}%
  }
  \caption{Cloud infrastructure architecture showing Supabase serverless components and data flow from edge device to application layer.}
  \label{fig:cloud_architecture}
\end{figure}

The data flow operates as follows: the Jetson edge device transmits JSON-formatted detection records over WiFi to the Supabase REST API. Records are validated and stored in the PostgreSQL database with automatic timestamps. Connected dashboards receive live updates through Realtime WebSocket subscriptions, enabling infrastructure managers to observe new pothole detections within seconds of capture.
